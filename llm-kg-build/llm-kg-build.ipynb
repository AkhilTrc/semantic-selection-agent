{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install unstructured\n",
    "!pip install libmagic\n",
    "!pip install unstructured[pdf]\n",
    "!pip install yachalk\n",
    "!pip install seaborn\n",
    "!pip install pyvis\n",
    "!pip isntall networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9c4c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3e154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.makedirs('./docs', exist_ok=True)\n",
    "from pyvis.network import Network\n",
    "graph_output_directory = \"./docs/index_LA.html\"\n",
    "\n",
    "LOG_FILE = './processed_files.log'\n",
    "\n",
    "import networkx as nx\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader, TextLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "palette = \"hls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    # Use this cell only if Chunking text\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "pages = splitter.split_documents(documents)\n",
    "print(\"Number of chunks = \", len(pages))\n",
    "print(pages[5].page_content)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"     Process to automate finding and incorporating new .txt files from a directory\n",
    "\n",
    "GRAPH_CSV_PATH = outputdirectory / \"graph.csv\"\n",
    "CHUNKS_CSV_PATH = outputdirectory / \"chunks.csv\"\n",
    "PROCESSED_LOG_PATH = \"processed_files.log\"\n",
    "\n",
    "## --- State Management: Identify New Files ---\n",
    "\n",
    "# Read the list of already processed files\n",
    "try:\n",
    "    with open(PROCESSED_LOG_PATH, 'r') as f:\n",
    "        processed_files = set(f.read().splitlines())\n",
    "except FileNotFoundError:\n",
    "    processed_files = set()\n",
    "\n",
    "# Get the list of all current .txt files in the directory\n",
    "current_files = {str(p) for p in inputdirectory.glob(\"**/*.txt\")}\n",
    "\n",
    "# Determine which files are new\n",
    "new_files_to_process = list(current_files - processed_files)\n",
    "\n",
    "print(f\"Found {len(new_files_to_process)} new file(s) to process.\")\n",
    "print(new_files_to_process)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d88ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Set\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "MODEL_NAME = 'gpt-4.1-nano'\n",
    "\n",
    "class PromptEngine:\n",
    "    def __init__(self):\n",
    "        self.cache: Dict[Tuple[str, str], str] = {}\n",
    "\n",
    "    def build(self, pairs: List[Tuple[str, str]]) -> List[Tuple[Tuple[str, str], str]]:\n",
    "        prompts = []\n",
    "        for e1, e2 in pairs:\n",
    "            if (e1, e2) not in self.cache:\n",
    "                prompt = f\"\"\"\n",
    "                You are a creative alchemist. Your job is to invent new elements by combining two existing ones in imaginative yet plausible ways.\n",
    "\n",
    "                When given two elements, combine their characteristics to create a new, unique element.\n",
    "                Express each result as:\n",
    "                `[Element1] and [Element2] gives me [ResultElement]`\n",
    "\n",
    "                **Rules:**\n",
    "\n",
    "                1. For each new element generated, attempt to combine it with each of the original input elements (the base set), but **do not** combine new elements with each other unless one is from the base set.\n",
    "                2. Continue this process for several generations, always only pairing a newly created element with any element from the original base set.\n",
    "                3. Each combination should produce a plausible new element, grounded in the properties or concepts of the ingredients.\n",
    "                4. Do not repeat combinations or reverse orderings (e.g., if \"Water and Fire\" is done, skip \"Fire and Water\").\n",
    "\n",
    "                **Template for output:**\n",
    "\n",
    "                ```\n",
    "                [Element1] and [Element2] gives me [ResultElement]\n",
    "                ```\n",
    "\n",
    "                **Input Elements:**\n",
    "\n",
    "                * {e1}\n",
    "                * {e2}\n",
    "\n",
    "                **Step-by-step:**\n",
    "\n",
    "                1. Combine {e1} and {e2} to create [O1].\n",
    "                2. Combine [O1] with {e1} (if not already used).\n",
    "                3. Combine [O1] with {e2} (if not already used).\n",
    "                4. For each new element produced, only combine with the original input elements, not with other new elements.\n",
    "\n",
    "                ---\n",
    "\n",
    "                **Example Workflow:**\n",
    "                Suppose Input Elements: **Fire** and **Water**\n",
    "\n",
    "                ```\n",
    "                Fire and Water gives me Steam\n",
    "                Steam and Fire gives me Energy\n",
    "                Steam and Water gives me Cloud\n",
    "                ```\n",
    "             \"\"\"\n",
    "                prompts.append(((e1, e2), prompt))\n",
    "        return prompts\n",
    "\n",
    "    def cache_result(self, pair: Tuple[str, str], result: str):\n",
    "        self.cache[pair] = result\n",
    "\n",
    "class LLMClient:\n",
    "    def __init__(self, model: str = MODEL_NAME):\n",
    "        self.model = model\n",
    "\n",
    "    def batch_query(self, prompts: List[str]) -> List[str]:\n",
    "        responses = []\n",
    "        for prompt in prompts:\n",
    "            resp = openai.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            text = resp.choices[0].message.content.strip()\n",
    "            responses.append(text)\n",
    "        return responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05eafa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def text2Dataframe(text_chunks) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for chunk in text_chunks:\n",
    "        row = {\n",
    "            \"text\": chunk.page_content,\n",
    "            **chunk.metadata,\n",
    "            \"chunk_id\": uuid.uuid4().hex,\n",
    "        }\n",
    "        rows = rows + [row]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e0032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def generate(model, system, user):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def graphPrompt(input: str, metadata={}, model=\"gpt-4.1-mini\"):\n",
    "    if model == None:\n",
    "        model = \"gpt-4.1-mini\"\n",
    "\n",
    "    # model_info = client.show(model_name=model)\n",
    "    # print( chalk.blue(model_info))\n",
    "\n",
    "    SYS_PROMPT = (\n",
    "        \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context text chunk (delimited by ```) Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts as per the context. \\n\"\n",
    "        \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
    "            \"\\tTerms may include object, entity, location, organization, person, \\n\"\n",
    "            \"\\tcondition, acronym, documents, service, concept, etc.\\n\"\n",
    "            \"\\tTerms should be as atomistic as possible\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
    "            \"\\tTerms can be related to many other terms\\n\\n\"\n",
    "        \"Thought 3: Find out the relation between each such related pair of terms. \\n\\n\"\n",
    "        \"Format your output as a list of json. Each element of the list contains a pair of terms\"\n",
    "        \"and the relation between them, like the following: \\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
    "        '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\\n'\n",
    "        \"   }, {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "    response = generate(model=model, system=SYS_PROMPT, user=USER_PROMPT)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "    except:\n",
    "        print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()\n",
    "    random.shuffle(p)\n",
    "    rows = []\n",
    "    group = 0\n",
    "    for community in communities:\n",
    "        color = p.pop()\n",
    "        group += 1\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
    "    df_colors = pd.DataFrame(rows)\n",
    "    return df_colors\n",
    "\n",
    "def df2Graph(dataframe: pd.DataFrame, model=None) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "def graph2Df(nodes_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    graph_dataframe = pd.DataFrame(nodes_list).replace(\" \", np.nan)\n",
    "    graph_dataframe = graph_dataframe.dropna(subset=[\"node_1\", \"node_2\"])\n",
    "    graph_dataframe[\"node_1\"] = graph_dataframe[\"node_1\"].apply(lambda x: x.lower())\n",
    "    graph_dataframe[\"node_2\"] = graph_dataframe[\"node_2\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return graph_dataframe\n",
    "\n",
    "def display_graph(G):\n",
    "    ## Display & Save Graph Visualization\n",
    "    #\n",
    "    net = Network(\n",
    "        notebook=False,\n",
    "        # bgcolor=\"#1a1a1a\",\n",
    "        cdn_resources=\"remote\",\n",
    "        height=\"900px\",\n",
    "        width=\"100%\",\n",
    "        select_menu=True,\n",
    "        # font_color=\"#cccccc\",\n",
    "        filter_menu=False,\n",
    "    )\n",
    "\n",
    "    net.from_nx(G)\n",
    "    # net.repulsion(node_distance=150, spring_length=400)\n",
    "    net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n",
    "    # net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n",
    "    net.show_buttons(filter_=[\"physics\"])\n",
    "\n",
    "    net.show(graph_output_directory, notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1127d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)\n",
    "    # Self join with chunk id as the key will create a link between terms occuring in the same text chunk.\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
    "    # drop self loops\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "    ## Group and count edges.\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n",
    "        .reset_index()\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    # Drop edges with 1 count\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"\n",
    "    return dfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a717f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_viz_builder(dfg):\n",
    "    nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    for node in nodes:\n",
    "        G.add_node(\n",
    "            str(node)\n",
    "        )\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for index, row in dfg.iterrows():\n",
    "        G.add_edge(\n",
    "            str(row[\"node_1\"]),\n",
    "            str(row[\"node_2\"]),\n",
    "            title=row[\"edge\"],\n",
    "            weight=row['count']/4\n",
    "        )\n",
    "\n",
    "    \"\"\"     Girvan Newman Algorithm & Community coloring / Can also be used for Stopping Condition\n",
    "\n",
    "    # Graph Communities & Coloring\n",
    "    communities_generator = nx.community.girvan_newman(G)\n",
    "    top_level_communities = next(communities_generator)\n",
    "    next_level_communities = next(communities_generator)\n",
    "    communities = sorted(map(sorted, next_level_communities))\n",
    "    print(\"Number of Communities = \", len(communities))\n",
    "    print(communities)\n",
    "    df_colors = colors2Community(communities)\n",
    "\n",
    "    for index, row in df_colors.iterrows():\n",
    "        G.nodes[row['node']]['group'] = row['group']\n",
    "        G.nodes[row['node']]['color'] = row['color']\n",
    "        G.nodes[row['node']]['size'] = G.degree[row['node']]\"\n",
    "    \"\"\"\n",
    "\n",
    "    display_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02c1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphBuilder:\n",
    "    def __init__(self):\n",
    "        self.graph = nx.Graph()\n",
    "        self.prompt_engine = PromptEngine()\n",
    "        self.llm = LLMClient()\n",
    "        self.seen_nodes: Set[str] = set()\n",
    "\n",
    "    def get_unexplored_nodes(self) -> List[str]:\n",
    "        return [n for n in self.graph.nodes() if n not in self.seen_nodes]\n",
    "\n",
    "    def run_loop(\n",
    "        self,\n",
    "        base_elements: List[str],\n",
    "        max_iters: int = 5,\n",
    "        batch_size: int = 2\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.base = [e.lower() for e in base_elements]\n",
    "        for node in self.base:\n",
    "            self.graph.add_node(node)\n",
    "        print(f\"Seeded graph with base elements: {base_elements}\")\n",
    "\n",
    "        for iteration in range(1, max_iters + 1):\n",
    "            to_explore = self.get_unexplored_nodes()[:batch_size]\n",
    "            if not to_explore:\n",
    "                print(\"No more new nodes ‚Äî stopping early.\")\n",
    "                break\n",
    "\n",
    "            print(f\"\\n--- Iteration {iteration}: exploring {to_explore} ---\")\n",
    "\n",
    "            pairs = [(new, base) \n",
    "                     for new in to_explore \n",
    "                     for base in self.base \n",
    "                     if new != base]\n",
    "\n",
    "            built = self.prompt_engine.build(pairs)\n",
    "            pairs_batch, prompts = zip(*built)\n",
    "            responses = self.llm.batch_query(list(prompts))\n",
    "\n",
    "            for (node, _), raw in zip(pairs_batch, responses):\n",
    "                print(f\"\\nüîÑ Response for '{node}':\\n{raw}\")\n",
    "                try:\n",
    "                    triples = self.preprocess_response(raw)\n",
    "                    text_chunks = raw.split('\\n')\n",
    "                    text_chunks = [chunk.strip() for chunk in text_chunks if chunk.strip()]\n",
    "                    df = text2Dataframe(text_chunks)\n",
    "                    concepts_list = df2Graph(df, model='gpt-4.1-mini')\n",
    "                    dfg1 = graph2Df(concepts_list)\n",
    "                    # if not os.path.exists(outputdirectory):\n",
    "                        # os.makedirs(outputdirectory)\n",
    "                    \n",
    "                    dfg1.to_csv(GRAPH_CSV_PATH, sep=\"|\", index=False)\n",
    "                    # df.to_csv(CHUNKS_CSV_PATH, sep=\"|\", index=False)\n",
    "                    print(f\"  Extracted triples: {triples}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ö†Ô∏è Could not parse JSON for '{node}'. Marking as seen.\")\n",
    "                    self.seen_nodes.add(node)\n",
    "                    continue\n",
    "\n",
    "                self.add_triples_to_graph(triples)\n",
    "                self.seen_nodes.add(node)\n",
    "\n",
    "            time.sleep(1)\n",
    "\n",
    "        print(\n",
    "            f\"\\nDone! Graph has {self.graph.number_of_nodes()} nodes \"\n",
    "            f\"and {self.graph.number_of_edges()} edges.\"\n",
    "        )\n",
    "        net = Network(notebook=True, height=\"800px\", width=\"100%\")\n",
    "        net.from_nx(self.graph) \n",
    "        net.show_buttons(filter_=['physics'])\n",
    "        net.show(\"kg_visualization_8iter.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = \"test_output1\"\n",
    "outputdirectory = Path(f\"./data_output/{out_dir}\")\n",
    "print(f\"Output directory: {outputdirectory}\")\n",
    "#print the file and its creation time\n",
    "print(f\"Output directory: {outputdirectory}\")\n",
    "import os\n",
    "print(os.listdir(outputdirectory))\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# Print the output directory\n",
    "print(f\"Output directory: {outputdirectory}\")\n",
    "\n",
    "# List files and print their creation times\n",
    "for filename in os.listdir(outputdirectory):\n",
    "    filepath = os.path.join(outputdirectory, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        creation_time = os.path.getctime(filepath)\n",
    "        readable_time = datetime.datetime.fromtimestamp(creation_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        print(f\"{filename} - Created on: {readable_time}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a2c529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_KG(text_files):\n",
    "    ## Dir PDF Loader example / adapt as needed\n",
    "    # loader = PyPDFDirectoryLoader(inputdirectory)\n",
    "    # loader = PyPDFLoader(\"./data/MedicalDocuments/orf-path_health-n1.pdf\")\n",
    "\n",
    "    # loader = DirectoryLoader(inputdirectory, show_progress=True)       # Load all text files in directory\n",
    "    # documents = loader.load()\n",
    "\n",
    "    out_dir = \"test_output1\"\n",
    "    outputdirectory = Path(f\"./data_output/{out_dir}\")\n",
    "    print(f\"Output directory: {outputdirectory}\")\n",
    "    if not os.path.exists(outputdirectory):\n",
    "        os.makedirs(outputdirectory)\n",
    "    text_files = [f.split(\"/\")[-1] for f in text_files if f.endswith('.txt')]\n",
    "    text_loader = TextLoader(f\"{text_files[0]}\")      # Assumedly, load just one text file in the directory\n",
    "    text_lines = text_loader.load()\n",
    "\n",
    "    # Split the text by the newline character\n",
    "    print(text_lines)\n",
    "    text_chunks = text_lines\n",
    "    #text_chunks = text_lines.split('\\n')\n",
    "    print(text_chunks)\n",
    "\n",
    "    # Convert text to dataframe\n",
    "    df = text2Dataframe(text_chunks)\n",
    "    print(df.shape)\n",
    "    df.head()\n",
    "\n",
    "    ## To Generate or Regenerate the Knowldge Graph with LLM \n",
    "    ## To Generate or Regenerate the Knowldge Graph with LLM \n",
    "    #\n",
    "    regenerate = True       # set this to 'False' for the Concatenation process       # set this to 'False' for the Concatenation process\n",
    "    GRAPH_CSV_PATH = outputdirectory / \"graph.csv\"\n",
    "    # # CHUNKS_CSV_PATH = outputdirectory / \"chunks.csv\"\n",
    "\n",
    "    if regenerate:      # I doubt we need regeneration      # I doubt we need regeneration\n",
    "        concepts_list = df2Graph(df, model='gpt-4.1-mini')\n",
    "        dfg1 = graph2Df(concepts_list)\n",
    "        if not os.path.exists(outputdirectory):\n",
    "            os.makedirs(outputdirectory)\n",
    "        \n",
    "        dfg1.to_csv(GRAPH_CSV_PATH, sep=\"|\", index=False)\n",
    "        # df.to_csv(CHUNKS_CSV_PATH, sep=\"|\", index=False)\n",
    "    else:\n",
    "        dfg1 = pd.read_csv(GRAPH_CSV_PATH, sep=\"|\")\n",
    "\n",
    "    # --- Load Existing Graph or Initialize Empty ---\n",
    "\n",
    "    #if os.path.exists(GRAPH_CSV_PATH):\n",
    "    #    print(\"Loading existing graph...\")\n",
    "    #    df_graph_existing = pd.read_csv(GRAPH_CSV_PATH, sep=\"|\")\n",
    "    #    df_chunks_existing = pd.read_csv(CHUNKS_CSV_PATH, sep=\"|\")\n",
    "#\n",
    "    #elif os.path.exists(GRAPH_CSV_PATH):\n",
    "    #    with open(LOG_FILE, 'r') as f:\n",
    "    #        content = f.read()\n",
    "    #        if not content:\n",
    "    #            print(\"LOG_FILE is empty.\")\n",
    "    #        else:\n",
    "    #            df_graph = pd.read_csv(GRAPH_CSV_PATH, sep=\"|\")\n",
    "    #            dfg1 = pd.concat([df_graph, dfg1], ignore_index=True)   # Concatenation process / still need modification\n",
    "    #    \n",
    "    #else:\n",
    "    #    print(\"No existing graph found. Starting fresh.\")\n",
    "    #    df_graph_existing = pd.DataFrame(columns=['node_1', 'node_2', 'edge', 'chunk_id'])\n",
    "    #    df_chunks_existing = pd.DataFrame(columns=['text', 'source', 'chunk_id'])\n",
    "\n",
    "        if os.path.exists(GRAPH_CSV_PATH):\n",
    "            print(\"Loading existing graph...\")\n",
    "            # df_chunks_existing = pd.read_csv(CHUNKS_CSV_PATH, sep=\"|\")\n",
    "\n",
    "            with open(LOG_FILE, 'r') as f:\n",
    "                content = f.read()\n",
    "                if not content:\n",
    "                    print(\"LOG_FILE is empty.\")\n",
    "                else:\n",
    "                    df_graph_existing = pd.read_csv(GRAPH_CSV_PATH, sep=\"|\")     # Reading the existing graph\n",
    "                    dfg1 = pd.concat([df_graph_existing, dfg1], ignore_index=True)   # Concatenation process / still need modification - esp. the text-chunking process  \n",
    "                    \n",
    "                    dfg1.replace(\"\", np.nan, inplace=True)\n",
    "                    dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "                    # Still confused about including 'count' column in this new implementation \n",
    "\n",
    "\n",
    "    ## Perform Contextual Proximity calculation / Use only if needed\n",
    "    #\n",
    "    dfg2 = contextual_proximity(dfg1)\n",
    "    dfg2.tail()\n",
    "    dfg2.to_csv(outputdirectory/\"context_prox_df.csv\", index=False)\n",
    "\n",
    "    ## Graph Concatenation & Aggregation\n",
    "    #\n",
    "    dfg = pd.concat([dfg1, dfg2], axis=0)\n",
    "    dfg = (\n",
    "        dfg.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    ## Graph-visualizier-building\n",
    "    \n",
    "    graph_viz_builder(dfg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \"\"\" Not performing textsplit-and-overlapping chunking process as of now, \n",
    "        bcs the inputs to this modification are just independent lines of text separated by newline character (\\n). \n",
    "        Also, only one new text file at a time. \n",
    "    \"\"\"\n",
    "    \"\"\" Not performing textsplit-and-overlapping chunking process as of now, \n",
    "        bcs the inputs to this modification are just independent lines of text separated by newline character (\\n). \n",
    "        Also, only one new text file at a time. \n",
    "    \"\"\"\n",
    "    import pathlib\n",
    "    first_run = True       # Change to ''False'' after the the first graph creation\n",
    "    INPUT_DIR = pathlib.Path('./data_input')\n",
    "    LOG_FILE = pathlib.Path(LOG_FILE)\n",
    "    if first_run:\n",
    "        if not os.listdir(INPUT_DIR):\n",
    "            print(\"Directory is empty\")\n",
    "        else:\n",
    "            text_files = [str(p) for p in INPUT_DIR.glob(\"**/*.txt\")]\n",
    "            print(text_files)\n",
    "            #text_files = \"\"\n",
    "            \n",
    "            # Processing files for the first time\n",
    "            if LOG_FILE.exists():\n",
    "                with open(LOG_FILE, 'r') as f:\n",
    "                    processed_files = set(f.read().splitlines())\n",
    "            else:\n",
    "                processed_files = set()     # Creating 'log file' if one not already created\n",
    "            \n",
    "            implement_KG(text_files)      # Generate/Concatenate Knowledge Graph\n",
    "\n",
    "            with open(LOG_FILE, 'a') as f:  # Logging new processed files\n",
    "                for file in text_files:\n",
    "                    f.write(file + '\\n')\n",
    "    else:\n",
    "        all_files = {str(p) for p in INPUT_DIR.glob(\"**/*.txt\")}\n",
    "        if LOG_FILE.exists():\n",
    "                with open(LOG_FILE, 'r') as f:\n",
    "                    processed_files = set(f.read().splitlines())\n",
    "\n",
    "        # Finding newly added files\n",
    "        new_files = list(all_files - processed_files)\n",
    "\n",
    "        if not new_files:\n",
    "            print(\"No new files to process.\")\n",
    "            # Optionally exit or continue as needed\n",
    "        else:\n",
    "            print(f\"Processing {len(new_files)} new files...\\n\")\n",
    "            implement_KG(new_files)        # Still need to modify this for improved Graph Concatenation\n",
    "\n",
    "            with open(LOG_FILE, 'a') as f:  # Logging new processed files\n",
    "                for file in all_files:\n",
    "                    f.write(file + '\\n')\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-kg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
