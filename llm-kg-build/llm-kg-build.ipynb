{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4b0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install openai\n",
    "!pip install langchain\n",
    "!pip install -U langchain-community\n",
    "!pip install unstructured\n",
    "!pip install libmagic\n",
    "!pip install unstructured[pdf]\n",
    "!pip install yachalk\n",
    "!pip install seaborn\n",
    "!pip install pyvis\n",
    "!pip isntall networkx\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3e154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.makedirs('./docs', exist_ok=True)\n",
    "from pyvis.network import Network\n",
    "graph_output_directory = \"./docs/index_IP.html\"\n",
    "\n",
    "import networkx as nx\n",
    "from langchain.document_loaders import PyPDFLoader, UnstructuredPDFLoader, PyPDFium2Loader, TextLoader\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import seaborn as sns\n",
    "palette = \"hls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    # Use this cell only if Chunking text\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=150,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "pages = splitter.split_documents(documents)\n",
    "print(\"Number of chunks = \", len(pages))\n",
    "print(pages[5].page_content)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef57c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"     Process to automate finding and incorporating new .txt files from a directory\n",
    "\n",
    "GRAPH_CSV_PATH = outputdirectory / \"graph.csv\"\n",
    "CHUNKS_CSV_PATH = outputdirectory / \"chunks.csv\"\n",
    "PROCESSED_LOG_PATH = \"processed_files.log\"\n",
    "\n",
    "## --- State Management: Identify New Files ---\n",
    "\n",
    "# Read the list of already processed files\n",
    "try:\n",
    "    with open(PROCESSED_LOG_PATH, 'r') as f:\n",
    "        processed_files = set(f.read().splitlines())\n",
    "except FileNotFoundError:\n",
    "    processed_files = set()\n",
    "\n",
    "# Get the list of all current .txt files in the directory\n",
    "current_files = {str(p) for p in inputdirectory.glob(\"**/*.txt\")}\n",
    "\n",
    "# Determine which files are new\n",
    "new_files_to_process = list(current_files - processed_files)\n",
    "\n",
    "print(f\"Found {len(new_files_to_process)} new file(s) to process.\")\n",
    "print(new_files_to_process)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05eafa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def text2Dataframe(text_chunks) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for chunk in text_chunks:\n",
    "        row = {\n",
    "            \"text\": chunk.page_content,\n",
    "            **chunk.metadata,\n",
    "            \"chunk_id\": uuid.uuid4().hex,\n",
    "        }\n",
    "        rows = rows + [row]\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54e0032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "client = OpenAI(api_key=API_KEY)\n",
    "\n",
    "def generate(model, system, user):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def graphPrompt(input: str, metadata={}, model=\"gpt-4.1-mini\"):\n",
    "    if model == None:\n",
    "        model = \"gpt-4.1-mini\"\n",
    "\n",
    "    # model_info = client.show(model_name=model)\n",
    "    # print( chalk.blue(model_info))\n",
    "\n",
    "    SYS_PROMPT = (\n",
    "        \"You are a network graph maker who extracts terms and their relations from a given context. \"\n",
    "        \"You are provided with a context text chunk (delimited by ```) Your task is to extract the ontology \"\n",
    "        \"of terms mentioned in the given context. These terms should represent the key concepts as per the context. \\n\"\n",
    "        \"Thought 1: While traversing through each sentence, Think about the key terms mentioned in it.\\n\"\n",
    "            \"\\tTerms may include object, entity, location, organization, person, \\n\"\n",
    "            \"\\tcondition, acronym, documents, service, concept, etc.\\n\"\n",
    "            \"\\tTerms should be as atomistic as possible\\n\\n\"\n",
    "        \"Thought 2: Think about how these terms can have one on one relation with other terms.\\n\"\n",
    "            \"\\tTerms can be related to many other terms\\n\\n\"\n",
    "        \"Thought 3: Find out the relation between each such related pair of terms. \\n\\n\"\n",
    "        \"Format your output as a list of json. Each element of the list contains a pair of terms\"\n",
    "        \"and the relation between them, like the following: \\n\"\n",
    "        \"[\\n\"\n",
    "        \"   {\\n\"\n",
    "        '       \"node_1\": \"A concept from extracted ontology\",\\n'\n",
    "        '       \"node_2\": \"A related concept from extracted ontology\",\\n'\n",
    "        '       \"edge\": \"relationship between the two concepts, node_1 and node_2 in one or two sentences\"\\n'\n",
    "        \"   }, {...}\\n\"\n",
    "        \"]\"\n",
    "    )\n",
    "\n",
    "    USER_PROMPT = f\"context: ```{input}``` \\n\\n output: \"\n",
    "    response = generate(model=model, system=SYS_PROMPT, user=USER_PROMPT)\n",
    "    try:\n",
    "        result = json.loads(response)\n",
    "        result = [dict(item, **metadata) for item in result]\n",
    "    except:\n",
    "        print(\"\\n\\nERROR ### Here is the buggy response: \", response, \"\\n\\n\")\n",
    "        result = None\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d4d643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colors2Community(communities) -> pd.DataFrame:\n",
    "    ## Define a color palette\n",
    "    p = sns.color_palette(palette, len(communities)).as_hex()\n",
    "    random.shuffle(p)\n",
    "    rows = []\n",
    "    group = 0\n",
    "    for community in communities:\n",
    "        color = p.pop()\n",
    "        group += 1\n",
    "        for node in community:\n",
    "            rows += [{\"node\": node, \"color\": color, \"group\": group}]\n",
    "    df_colors = pd.DataFrame(rows)\n",
    "    return df_colors\n",
    "\n",
    "def df2Graph(dataframe: pd.DataFrame, model=None) -> list:\n",
    "    # dataframe.reset_index(inplace=True)\n",
    "    results = dataframe.apply(\n",
    "        lambda row: graphPrompt(row.text, {\"chunk_id\": row.chunk_id}, model), axis=1\n",
    "    )\n",
    "    # invalid json results in NaN\n",
    "    results = results.dropna()\n",
    "    results = results.reset_index(drop=True)\n",
    "\n",
    "    ## Flatten the list of lists to one single list of entities.\n",
    "    concept_list = np.concatenate(results).ravel().tolist()\n",
    "    return concept_list\n",
    "\n",
    "def graph2Df(nodes_list) -> pd.DataFrame:\n",
    "    ## Remove all NaN entities\n",
    "    graph_dataframe = pd.DataFrame(nodes_list).replace(\" \", np.nan)\n",
    "    graph_dataframe = graph_dataframe.dropna(subset=[\"node_1\", \"node_2\"])\n",
    "    graph_dataframe[\"node_1\"] = graph_dataframe[\"node_1\"].apply(lambda x: x.lower())\n",
    "    graph_dataframe[\"node_2\"] = graph_dataframe[\"node_2\"].apply(lambda x: x.lower())\n",
    "\n",
    "    return graph_dataframe\n",
    "\n",
    "def display_graph(G):\n",
    "    ## Display & Save Graph Visualization\n",
    "    #\n",
    "    net = Network(\n",
    "        notebook=False,\n",
    "        # bgcolor=\"#1a1a1a\",\n",
    "        cdn_resources=\"remote\",\n",
    "        height=\"900px\",\n",
    "        width=\"100%\",\n",
    "        select_menu=True,\n",
    "        # font_color=\"#cccccc\",\n",
    "        filter_menu=False,\n",
    "    )\n",
    "\n",
    "    net.from_nx(G)\n",
    "    # net.repulsion(node_distance=150, spring_length=400)\n",
    "    net.force_atlas_2based(central_gravity=0.015, gravity=-31)\n",
    "    # net.barnes_hut(gravity=-18100, central_gravity=5.05, spring_length=380)\n",
    "    net.show_buttons(filter_=[\"physics\"])\n",
    "\n",
    "    net.show(graph_output_directory, notebook=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1127d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_proximity(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ## Melt the dataframe into a list of nodes\n",
    "    dfg_long = pd.melt(\n",
    "        df, id_vars=[\"chunk_id\"], value_vars=[\"node_1\", \"node_2\"], value_name=\"node\"\n",
    "    )\n",
    "    dfg_long.drop(columns=[\"variable\"], inplace=True)\n",
    "    # Self join with chunk id as the key will create a link between terms occuring in the same text chunk.\n",
    "    dfg_wide = pd.merge(dfg_long, dfg_long, on=\"chunk_id\", suffixes=(\"_1\", \"_2\"))\n",
    "    # drop self loops\n",
    "    self_loops_drop = dfg_wide[dfg_wide[\"node_1\"] == dfg_wide[\"node_2\"]].index\n",
    "    dfg2 = dfg_wide.drop(index=self_loops_drop).reset_index(drop=True)\n",
    "    ## Group and count edges.\n",
    "    dfg2 = (\n",
    "        dfg2.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": [\",\".join, \"count\"]})\n",
    "        .reset_index()\n",
    "    )\n",
    "    dfg2.columns = [\"node_1\", \"node_2\", \"chunk_id\", \"count\"]\n",
    "    dfg2.replace(\"\", np.nan, inplace=True)\n",
    "    dfg2.dropna(subset=[\"node_1\", \"node_2\"], inplace=True)\n",
    "    # Drop edges with 1 count\n",
    "    dfg2 = dfg2[dfg2[\"count\"] != 1]\n",
    "    dfg2[\"edge\"] = \"contextual proximity\"\n",
    "    return dfg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a717f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_viz_builder(dfg):\n",
    "    nodes = pd.concat([dfg['node_1'], dfg['node_2']], axis=0).unique()\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes to the graph\n",
    "    for node in nodes:\n",
    "        G.add_node(\n",
    "            str(node)\n",
    "        )\n",
    "\n",
    "    # Add edges to the graph\n",
    "    for index, row in dfg.iterrows():\n",
    "        G.add_edge(\n",
    "            str(row[\"node_1\"]),\n",
    "            str(row[\"node_2\"]),\n",
    "            title=row[\"edge\"],\n",
    "            weight=row['count']/4\n",
    "        )\n",
    "\n",
    "    \"\"\"     Girvan Newman Algorithm & Community coloring / Can also be used for Stopping Condition\n",
    "\n",
    "    # Graph Communities & Coloring\n",
    "    communities_generator = nx.community.girvan_newman(G)\n",
    "    top_level_communities = next(communities_generator)\n",
    "    next_level_communities = next(communities_generator)\n",
    "    communities = sorted(map(sorted, next_level_communities))\n",
    "    print(\"Number of Communities = \", len(communities))\n",
    "    print(communities)\n",
    "    df_colors = colors2Community(communities)\n",
    "\n",
    "    for index, row in df_colors.iterrows():\n",
    "        G.nodes[row['node']]['group'] = row['group']\n",
    "        G.nodes[row['node']]['color'] = row['color']\n",
    "        G.nodes[row['node']]['size'] = G.degree[row['node']]\"\n",
    "    \"\"\"\n",
    "\n",
    "    display_graph(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc79fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement(text_files):\n",
    "    ## Dir PDF Loader example / adapt as needed\n",
    "    # loader = PyPDFDirectoryLoader(inputdirectory)\n",
    "    # loader = PyPDFLoader(\"./data/MedicalDocuments/orf-path_health-n1.pdf\")\n",
    "\n",
    "    # loader = DirectoryLoader(inputdirectory, show_progress=True)       # Load all text files in directory\n",
    "    # documents = loader.load()\n",
    "\n",
    "    out_dir = \"test_output1\"\n",
    "    outputdirectory = Path(f\"./data_output/{out_dir}\")\n",
    "    if not os.path.exists(outputdirectory):\n",
    "        os.makedirs(outputdirectory)\n",
    "\n",
    "    text_loader = TextLoader(f\"./data_input/{text_files[0]}\", show_progress=True)      # Load just one text file in directory\n",
    "    text_lines = text_loader.load()\n",
    "\n",
    "    # Split the text by the newline character\n",
    "    text_chunks = text_lines.split('\\n')\n",
    "    print(text_chunks)\n",
    "\n",
    "    # Convert text to dataframe\n",
    "    df = text2Dataframe(text_chunks)\n",
    "    print(df.shape)\n",
    "    df.head()\n",
    "\n",
    "    ## To Generate or Regenerate the Knowldge Graph with LLM / set this to True\n",
    "    #\n",
    "    regenerate = True\n",
    "    GRAPH_CSV_PATH = outputdirectory / \"graph.csv\"\n",
    "    CHUNKS_CSV_PATH = outputdirectory / \"chunks.csv\"\n",
    "\n",
    "    if regenerate:\n",
    "        concepts_list = df2Graph(df, model='gpt-4.1-mini')\n",
    "        dfg1 = graph2Df(concepts_list)\n",
    "        if not os.path.exists(outputdirectory):\n",
    "            os.makedirs(outputdirectory)\n",
    "        \n",
    "        dfg1.to_csv(GRAPH_CSV_PATH, sep=\"|\", index=False)\n",
    "        df.to_csv(CHUNKS_CSV_PATH, sep=\"|\", index=False)\n",
    "    else:\n",
    "        dfg1 = pd.read_csv(GRAPH_CSV_PATH, sep=\"|\")\n",
    "\n",
    "    # --- Load Existing Graph or Initialize Empty ---\n",
    "\n",
    "    if os.path.exists(GRAPH_CSV_PATH):\n",
    "        print(\"Loading existing graph...\")\n",
    "        df_graph_existing = pd.read_csv(GRAPH_CSV_PATH, sep=\"|\")\n",
    "        df_chunks_existing = pd.read_csv(CHUNKS_CSV_PATH, sep=\"|\")\n",
    "    else:\n",
    "        print(\"No existing graph found. Starting fresh.\")\n",
    "        df_graph_existing = pd.DataFrame(columns=['node_1', 'node_2', 'edge', 'chunk_id'])\n",
    "        df_chunks_existing = pd.DataFrame(columns=['text', 'source', 'chunk_id'])\n",
    "\n",
    "\n",
    "    dfg1.replace(\"\", np.nan, inplace=True)\n",
    "    dfg1.dropna(subset=[\"node_1\", \"node_2\", 'edge'], inplace=True)\n",
    "    dfg1['count'] = 4       ## Increasing the weight of the relation to 4. \n",
    "                                        ## We will assign the weight of 1 when later the contextual proximity will be calculated.  \n",
    "    print(dfg1.shape)\n",
    "    dfg1.head()\n",
    "\n",
    "\n",
    "    ## Perform Contextual Proximity calculation / Use only if needed\n",
    "    #\n",
    "    dfg2 = contextual_proximity(dfg1)\n",
    "    dfg2.tail()\n",
    "    dfg2.to_csv(outputdirectory/\"context_prox_df.csv\", index=False)\n",
    "\n",
    "    ## Graph Concatenation & Aggregation\n",
    "    #\n",
    "    dfg = pd.concat([dfg1, dfg2], axis=0)\n",
    "    dfg = (\n",
    "        dfg.groupby([\"node_1\", \"node_2\"])\n",
    "        .agg({\"chunk_id\": \",\".join, \"edge\": ','.join, 'count': 'sum'})\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    ## Graph-visualizier-building\n",
    "    #\n",
    "    graph_viz_builder(dfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd575cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    first_run = True       # Change to False after first graph creation\n",
    "    INPUT_DIR = './data_input'\n",
    "    LOG_FILE = './processed_files.log'    \n",
    "\n",
    "    if first_run:\n",
    "        if not os.listdir(INPUT_DIR):\n",
    "            print(\"Directory is empty\")\n",
    "        else:\n",
    "            text_files = {str(p) for p in INPUT_DIR.glob(\"**/*.txt\")}\n",
    "            \n",
    "            # Processing files for first time\n",
    "            if LOG_FILE.exists():\n",
    "                with open(LOG_FILE, 'r') as f:\n",
    "                    processed_files = set(f.read().splitlines())\n",
    "            else:\n",
    "                processed_files = set()\n",
    "            \n",
    "            implement(text_files)\n",
    "\n",
    "            with open(LOG_FILE, 'a') as f:  # Logging new processed files\n",
    "                for file in text_files:\n",
    "                    f.write(file + '\\n')\n",
    "    else:\n",
    "        all_files = {str(p) for p in INPUT_DIR.glob(\"**/*.txt\")}\n",
    "        if LOG_FILE.exists():\n",
    "                with open(LOG_FILE, 'r') as f:\n",
    "                    processed_files = set(f.read().splitlines())\n",
    "\n",
    "        # Finding newly added files\n",
    "        new_files = list(all_files - processed_files)\n",
    "\n",
    "        if not new_files:\n",
    "            print(\"No new files to process.\")\n",
    "            # Optionally exit or continue as needed\n",
    "        else:\n",
    "            print(f\"Processing {len(new_files)} new files...\\n\")\n",
    "            implement(new_files)        # Still need to update this for Graph Concatenation\n",
    "\n",
    "            with open(LOG_FILE, 'a') as f:  # Logging new processed files\n",
    "                for file in all_files:\n",
    "                    f.write(file + '\\n')\n",
    "\n",
    "    print(\"\\nDone.\")\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-kg-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
